<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://lurenss.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lurenss.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-08T18:10:58+00:00</updated><id>https://lurenss.github.io/feed.xml</id><title type="html">blank</title><subtitle>:)</subtitle><entry><title type="html">Beyond chatbots</title><link href="https://lurenss.github.io/blog/2024/More-than-chatbots/" rel="alternate" type="text/html" title="Beyond chatbots"/><published>2024-01-14T21:01:00+00:00</published><updated>2024-01-14T21:01:00+00:00</updated><id>https://lurenss.github.io/blog/2024/More-than-chatbots</id><content type="html" xml:base="https://lurenss.github.io/blog/2024/More-than-chatbots/"><![CDATA[<p>Even the least tech-savvy person can see that something significant has occurred in the last year, the release of ChatGPT on 30-12-2022 to the public and the consequent mass usage is likely to pass from a small fraction of people that play a video game like tetris to the millions of people that play a more advanced videgame like GTA 6 (hopefully in 2025 if Rockstar doesn’t make treats :) ) The only difference is the gaming field this took more than 30 years, in the AI field was a lightning strike.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/tris-480.webp 480w, /assets/img/tris-800.webp 800w, /assets/img/tris-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/tris.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/gta6-480.webp 480w, /assets/img/gta6-800.webp 800w, /assets/img/gta6-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/gta6.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Tris on the left, GTA 6 on the right, the gap is huge </div> <p>As a consequence of this,the AI field boiled up once again, and a new gold rush started. It’s important to mention that not only big corporations are involved in this, but also small startups and even single developers of the open source community, fortunatly i say, It’s important to avoid the monopoly of big corporations in this field, but this will be maybe topic for another post.</p> <h2 id="how-this-stuff-works-on-high-level">How this stuff works on high level</h2> <p>The underlying technology that powers up LLMs is called Transformers, it was invented by Google in 2017 (here the <a href="[https://arxiv.org/abs/1706.03762]">link</a> for the paper) <br/> In order to understand let’s assume that our training set is only a single phrase with the following words “I like eat pizza with Marco” and not the entire wikipedia, reddit as did OpenAI. So basically what does a transformer is given a sequence of word guess the next one, that’s it. <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/transformer1-480.webp 480w, /assets/img/transformer1-800.webp 800w, /assets/img/transformer1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At the beginning the transformer it doesn’t know anything so it decide to put a random word.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/transformer2-480.webp 480w, /assets/img/transformer2-800.webp 800w, /assets/img/transformer2-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>At this point the learning algorithm say to the transformer, Hey jackass you don’t have to insert toilet after I like eat but pizza. So the next time when I ask what is the word that comes after I like eat it will answer with pizza.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/transformer3-480.webp 480w, /assets/img/transformer3-800.webp 800w, /assets/img/transformer3-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>So now this new sequence becomes the input of the transformer and it has to guess the next word, so the previous process is iterated and so on.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/transformer4-480.webp 480w, /assets/img/transformer4-800.webp 800w, /assets/img/transformer4-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/transformer4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This is just what does the transform predict the next word, or in Natural Language Processing terms predict the next token, just it. More text see the transformer and more will be able to produce sophisticated text given a certain sequence.</p> <h2 id="beyond-the-text">Beyond the text</h2> <p>Until now we have seen how the transformer can be used to generate text, but what if we want to use it for something else? <br/> Well It’s simple instead of using word as tokens we can use different things like GPS coordinates, DNA sequences and so on. Let’s take Transportation field where the ability to forecast people trajectories can greatly improve the efficiency and effectiveness of transportation systems. With the availability of a vast dataset of GPS coordinates, the transformer model can be utilized to predict the next GPS coordinate of an individual or group of people based on a sequence of previous coordinates. This advanced predictive capability has the potential to optimize transportation routes, reduce travel time, and enhance overall user experience. Alternatively, this technique can also be used to produce synthetic data in instances where it is necessary to achieve a specific population size and analyze its characteristics, based on a representative dataset. Creating another parallelism between videogames and real life, with a transformer based model trained on user GPS coordinates, is tool that is mimicking the actions and behaviors of individuals based on their history, similar to how city builders or management games function, when the player is aware of traffic congestion, He can take the necessary steps to avoid it, like insert a new road or a new bus line.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/citiesskylin-480.webp 480w, /assets/img/citiesskylin-800.webp 800w, /assets/img/citiesskylin-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/citiesskylin.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A screenshot from Cities Skylines, a city builder game </div> <p>What I’m saying is not just something I dreamed up - there are people who are working on it and have shown preliminary results, like in this papers <a href="[(https://www.frontiersin.org/articles/10.3389/fphy.2022.1021176/full)]">Generation of individual daily trajectories by GPT-2</a> and <a href="[(https://arxiv.org/pdf/2308.07940.pdf)]">Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data</a>. Instead for DNA sequences there are a lot of applications, for example, we can use it to predict the next nucleotide in a sequence, or to predict the next amino acid in a protein sequence, or to predict the next codon in a DNA sequence. So it is useful for task like sequence analysis, gene expression, here a paper that cover how transformers can be used in this field <a href="[(https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad001/6984737)]">Transformer-based models for gene expression prediction</a>.</p> <p>In conclusion, the transformer architecture has revolutionized the field of NLP, allowing for the creation of LLMs with the ability to simulate human-like conversation. Through the use of transformers, language models can be trained on vast datasets, allowing for the prediction of the next word or token based on a sequence of previous words. However, the applications of transformers extend beyond chatbots and language generation. Its ability to process and predict various forms of data, such as GPS coordinates and DNA sequences, opens up a world of possibilities in different fields. While there is still much to explore and develop in this area, the potential for transformers is evident and one technology has a great impact in various and different fields it’s for sure a sign of that is and it will be more and more important in the future.</p>]]></content><author><name></name></author><category term="post"/><category term="transformers"/><category term="AI"/><category term="breakthrough"/><category term="chatbots"/><summary type="html"><![CDATA[Transformers the AI breakthrough]]></summary></entry></feed>